# -*- coding: utf-8 -*-
"""cognee_taxi_dataset_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBA9OIGChcKjjg8r5hHduR0v3A5D6rmH

# **From REST to reasoning: ingest, index, and query with dlt and Cognee.**

## **Taxi dataset example**

## **Let's install Cognee and Kuzu**

- [Cognee](https://www.cognee.ai/) is an open-source python library, connects data points and establishes ground truths to improve the accuracy of your AI agents and LLMs.
- [Kuzu](https://kuzudb.com/) is an open-source embedded, scalable, blazing fast graph database.

Read more about Cognee and Kuzu here: https://blog.kuzudb.com/post/cognee-kuzu-relational-data-to-knowledge-graph/



# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install cognee
# !pip install kuzu

"""Before running Cognee you have to specify up your environment.

Cognee relies on third-party LLM providers and you have a [great choice of them](https://docs.cognee.ai/how-to-guides/remote-models) you can use in your workflow.

**The simple way**

Just provide your OpenAI API key if you already have one. This will help you both with LLM and embeddings.
"""

from google.colab import userdata
import os

os.environ["LLM_API_KEY"] = userdata.get('LLM_API_KEY') # it can be OpenAI API key
os.environ["GRAPH_DATABASE_PROVIDER"] = "kuzu"

"""## **Data we'll be using**

In this example, weâ€™ll request data from an API that serves the **NYC taxi dataset**. For these purposes we created an API that can serve the data you are already familiar with.

### **API documentation**:  
- **Data**: Comes in pages of 1,000 records.  
- **Pagination**: When thereâ€™s no more data, the API returns an empty page.  
- **Details**:  
  - **Method**: GET  
  - **URL**: `https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api`  
  - **Parameters**:  
    - `page`: Integer (page number), defaults to 1.  

Hereâ€™s how we design our requester:  
1. **Request page by page** until we hit an empty page. Since we donâ€™t know how much data is behind the API, we must assume it could be as little as 1,000 records or as much as 10GB.
2. **Use a generator** to handle this efficiently and avoid loading all data into memory.

The dates for taxi rides are all from within June 2009.

## **We'll be partitioning our data in our own way**

1. first_10_days
2. second_10_days
3. last_10_days

We'll be doing this manually for clarity, but dlt also supports partitioning, as you can find [here](https://dlthub.com/docs/plus/ecosystem/iceberg#partitioning).
"""

import dlt
import requests
import pandas as pd
from datetime import datetime

# Step 1: Create DLT resource
@dlt.resource(write_disposition="replace", name="zoomcamp_data")
def zoomcamp_data():
    url = "https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api"
    response = requests.get(url)
    data = response.json()

    # Convert to DataFrame
    df = pd.DataFrame(data)
    df['Trip_Pickup_DateTime'] = pd.to_datetime(df['Trip_Pickup_DateTime'])

    # Define buckets
    df['tag'] = pd.cut(
        df['Trip_Pickup_DateTime'],
        bins=[
            pd.Timestamp("2009-06-01"),
            pd.Timestamp("2009-06-10"),
            pd.Timestamp("2009-06-20"),
            pd.Timestamp("2009-06-30")
        ],
        labels=["first_10_days", "second_10_days", "last_10_days"],
        right=False
    )

    # Drop rows not in the specified range
    df = df[df['tag'].notnull()]
    yield df


# Step 2: Create and run the pipeline
pipeline = dlt.pipeline(
    pipeline_name="zoomcamp_pipeline",
    destination="duckdb",
    dataset_name="zoomcamp_tagged_data"
)
load_info = pipeline.run(zoomcamp_data())
print(pipeline.last_trace)

dataset = pipeline.dataset().zoomcamp_data.df()

dataset

dataset["tag"].value_counts()

"""## **Lets load data into Cognee!**

Here, I am using `cognee.add()` and then `cognee.cognify()` directly.

If you'd like to learn about how to use relational datasets in cognee, please visit the [docs](https://docs.cognee.ai/tutorials/load-your-relational-database) :)

import cognee
from cognee.shared.logging_utils import get_logger, ERROR
from cognee.api.v1.visualize.visualize import visualize_graph
from cognee.api.v1.search import SearchType
from cognee.modules.engine.models import NodeSet
import os

async def main():
    await cognee.prune.prune_data()
    await cognee.prune.prune_system(metadata=True)

    # Add the first 10 days
    df_set1 = dataset.loc[dataset["tag"] == "first_10_days"]
    df_set1.drop(columns=["tag"], inplace=True)
    df_set1 = df_set1.to_json(orient="records", lines=False)
    await cognee.add(df_set1, node_set=["first_10_days"])

    # Add the second 10 days
    df_set2 = dataset.loc[dataset["tag"] == "second_10_days"]
    df_set2.drop(columns=["tag"], inplace=True)
    df_set2 = df_set2.to_json(orient="records", lines=False)
    await cognee.add(df_set2, node_set=["second_10_days"])

    # Add the last 10 days
    df_set3 = dataset.loc[dataset["tag"] == "last_10_days"]
    df_set3.drop(columns=["tag"], inplace=True)
    df_set3 = df_set3.to_json(orient="records", lines=False)
    await cognee.add(df_set3, node_set=["last_10_days"])

    await cognee.cognify()

    visualization_path = "/content/.artifacts/graph_visualization.html"
    await visualize_graph(visualization_path)

"""Run the `main()` function. It might take some time, so go ahead and grab a cup of coffee. ðŸ«–"""

await main()

"""### **Look at the knowledge graph**


"""

from google.colab import files

files.download("/content/.artifacts/graph_visualization.html")

"""## **Search**"""

async def search_cognee(query, node_set, query_type=SearchType.GRAPH_COMPLETION):
    answer = await cognee.search(
        query_text=query,
        query_type=query_type,
        node_type=NodeSet,
        node_name=node_set,
        top_k=5 # limit search for retrieval
    )
    return answer

results = await search_cognee(
    "What's in this knowledge graph?",
    node_set=["first_10_days"]
)

print(results[0])