from fastapi import FastAPI, HTTPException
from typing import List
import httpx
import logging
import datetime

from schema import Message, ChatCompletionRequest

app = FastAPI()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ollama_api")

OLLAMA_API_URL = "http://ollama:11434/api/generate"


@app.post("/v1/chat/completions")
async def chat_completion(req: ChatCompletionRequest):
    try:
        prompt = build_prompt(req.messages)

        # ç¬¬ä¸€æ¬¡å‘¼å« LLM
        response_text = await call_ollama(req.model, prompt)

        # æ¨¡æ“¬ function trigger
        if "ç¾åœ¨å¹¾é»" in response_text:
            function_result = await get_current_time()

            # append function call + function response
            new_messages = req.messages + [
                Message(role="assistant", content=None, tool_call_id="tool-1", name="get_current_time"),
                Message(role="function", name="get_current_time", content=function_result)
            ]

            new_prompt = build_prompt(new_messages)
            final_response = await call_ollama(req.model, new_prompt)

            return {
                "id": "chatcmpl-001",
                "object": "chat.completion",
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": final_response
                    },
                    "finish_reason": "stop"
                }]
            }

        return {
            "id": "chatcmpl-001",
            "object": "chat.completion",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }]
        }

    except Exception as e:
        logger.error(f"éŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail="è™•ç†å¤±æ•—")


# ğŸ§  Prompt çµ„è£å™¨
def build_prompt(messages: List[Message]) -> str:
    result = ""
    for msg in messages:
        if msg.role == "user":
            result += f"User: {msg.content}\n"
        elif msg.role == "assistant":
            result += f"Assistant: {msg.content or ''}\n"
        elif msg.role == "function":
            result += f"(Function {msg.name} å›å‚³): {msg.content}\n"
    return result + "Assistant:"


# ğŸ§  å‘¼å« Ollama ç”¢ç”Ÿå›æ‡‰
async def call_ollama(model: str, prompt: str) -> str:
    async with httpx.AsyncClient(timeout=60) as client:
        res = await client.post(OLLAMA_API_URL, json={
            "model": model,
            "prompt": prompt,
            "stream": False  # âœ… ç¢ºä¿é streaming æ¨¡å¼ï¼Œå›å‚³ç‚ºå–®ç­† JSON
        })
        res.raise_for_status()

        # å¦‚æœä»ç„¶å›å‚³å¤šç­† JSONï¼ˆstream false ç„¡æ•ˆï¼‰ï¼Œæ‰‹å‹•è™•ç†
        try:
            return res.json().get("response", "")
        except Exception:
            # å¤šç­† JSONï¼Œç”¨ splitlines + å–æœ€å¾Œä¸€ç­†
            lines = res.text.strip().splitlines()
            for line in reversed(lines):
                if line.strip().startswith("{"):
                    try:
                        import json
                        obj = json.loads(line)
                        return obj.get("response", "")
                    except Exception:
                        continue
            raise ValueError("ç„¡æ³•è§£æ Ollama å›æ‡‰")


# ğŸ›  Function call æ¨¡æ“¬å‡½å¼
async def get_current_time() -> str:
    now = datetime.datetime.now().isoformat()
    return f"ç¾åœ¨æ™‚é–“æ˜¯ {now}"
